{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from langchain.document_loaders import UnstructuredFileLoader\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "from langchain.vectorstores import Chroma\n",
    "from pdf2image import convert_from_path\n",
    "import pytesseract\n",
    "\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "groq_api_key= os.getenv(\"GROQ_API_KEY\")\n",
    "open_ai_key= os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "from langchain.schema import Document\n",
    "pdf_path = \"C:/Users/hp/Downloads/PhilipsPC.pdf\" ## I am loading from this path\n",
    "persist_directory = \"C:/Users/hp/Downloads/chroma_embeddings\"\n",
    "\n",
    "# Step 1: Convert PDF to images\n",
    "images = convert_from_path(pdf_path)  # Ensure Poppler is installed and PATH is correct\n",
    "\n",
    "# Step 2: Convert extracted text to Document objects in to list in a for loop\n",
    "documents = []\n",
    "for i, image in enumerate(images):\n",
    "    text = pytesseract.image_to_string(image)\n",
    "    if text.strip():  # Skip empty pages\n",
    "        doc = Document(\n",
    "            page_content=text.strip(),  \n",
    "            metadata={\"page\": i + 1}  \n",
    "        )\n",
    "        documents.append(doc)\n",
    "\n",
    "print(f\"Extracted {len(documents)} documents:\")\n",
    "for doc in documents:\n",
    "    print(doc)\n",
    "\n",
    "# Step 3: Split text into manageable chunks\n",
    "try:\n",
    "    text_splitter = RecursiveCharacterTextSplitter(chunk_size=5000, chunk_overlap=500)\n",
    "    splits = text_splitter.split_documents(documents)\n",
    "except Exception as e:\n",
    "    print(\"Error during text splitting:\", e)\n",
    "    raise\n",
    "\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "from langchain.vectorstores import Chroma\n",
    "\n",
    "# Step 4: Create embeddings\n",
    "embeddings = OpenAIEmbeddings()\n",
    "\n",
    "# Step 5: Store the documents and embeddings in a Chroma vector database\n",
    "vector_store = Chroma.from_documents(splits, embeddings, persist_directory=persist_directory)\n",
    "\n",
    "\n",
    "\n",
    "retreiver=vector_store.as_retriever()\n",
    "from langchain.tools.retriever import create_retriever_tool\n",
    "retreivertool= create_retriever_tool(retreiver,\"Trimmer search\", \"search what i need\")\n",
    "from langchain_community.utilities import ArxivAPIWrapper,WikipediaAPIWrapper\n",
    "from langchain_community.tools import ArxivQueryRun,WikipediaQueryRun\n",
    " \n",
    "wikiwrapper = WikipediaAPIWrapper(top_k_results=1,doc_content_chars_max=250)\n",
    "wiki= WikipediaQueryRun(api_wrapper=wikiwrapper)\n",
    "\n",
    "\n",
    "from langchain import hub\n",
    "tools= [wiki,retreivertool]\n",
    "tools\n",
    "##we created a tool, lets use now model with groqapi\n",
    "from langchain_groq import ChatGroq\n",
    "model=ChatGroq(model=\"Gemma2-9b-It\",groq_api_key=groq_api_key)\n",
    "\n",
    "prompt=hub.pull(\"hwchase17/openai-functions-agent\")\n",
    "\n",
    "from langchain.agents import create_openai_tools_agent\n",
    "agent = create_openai_tools_agent(model,tools,prompt)\n",
    "from langchain.agents import AgentExecutor\n",
    "agent_executor=AgentExecutor(agent=agent,tools=tools,verbose=True)\n",
    "agent_executor\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
